{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36a2146",
   "metadata": {},
   "source": [
    "이름: 나영훈\n",
    "\n",
    "학번: 20185124"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c04dd",
   "metadata": {},
   "source": [
    "# Neural networks with PyTorch\n",
    "\n",
    "Pytorch의 `nn.module`을 활용하여 만드는 유용한 방법을 학습합니다.\n",
    "\n",
    "<div style=\"text-align:center\"><img src='https://drive.google.com/uc?export=download&id=1J2SeiPpVJs1-ML2BdLrcxkGGmHpRxIVE' width=\"250\" height=\"200\"> \n",
    "\n",
    "### Lego block coding! </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd06918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2e439aa2370>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "#ReLU, sigmoid .. ect 를 사용하기 위해 import torch.nn.functional as F 를 부름 (nn 사용도 가능)\n",
    "from collections import OrderedDict\n",
    "#OrderedDict : 순서가 있는 Dictionary\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605a354",
   "metadata": {},
   "source": [
    "`nn.Linear`: $Z^{[\\ell]} = A^{[\\ell-1]}W^T+b$\n",
    "연산.\n",
    "\n",
    "해당 layer의 \n",
    "\n",
    "- 입력 차원 `n_input=30`\n",
    "- 출력 차원 `n_output=60`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe1c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of nn.linear\n",
    "linear_layer1 = nn.Linear(30, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4a14fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 60])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(60, 30) \n",
    "linear_layer1(A).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534a281-b898-4ecc-838f-46b4a8f674ad",
   "metadata": {},
   "source": [
    "Why 60, 60 -> (60,30) x (30,60) = (60, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bcbcbf",
   "metadata": {},
   "source": [
    "How to get the weights and bias of each `nn.Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e636fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of weights\n",
    "linear_layer1.weight.data = torch.ones_like(linear_layer1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "214864da-24f2-454f-8f90-3db57cfefa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "430c221c-4532-46c7-9957-bcbda741ce23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer1.bias.shape\n",
    "#bias의 shape은 항상 1차원이다. 또한 bias의 크기는 output과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a96b2d",
   "metadata": {},
   "source": [
    "### NN example\n",
    "\n",
    "- input units: 20\n",
    "- hidden layer: 30, 40\n",
    "- output units: 3\n",
    "- activation function: ReLU\n",
    "- output layer: No activation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c09e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple NN construction\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lin1 = nn.Linear(20, 30)\n",
    "        self.lin2 = nn.Linear(30, 40)\n",
    "        self.lin3 = nn.Linear(40, 3)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab31bdc-5cb8-41b0-8be9-7a1d6330a7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 3])\n"
     ]
    }
   ],
   "source": [
    "Xtrain = torch.randn(60, 20)\n",
    "\n",
    "model = FCN()\n",
    "# print(model(Xtrain)) # print(model.forward(FCN()) 여기서 method의 이름이 forward이면 생략 가능함.\n",
    "print(model(Xtrain).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd3c910b-aaf2-4b2d-b1b6-6be5d55f593e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN(\n",
       "  (lin1): Linear(in_features=20, out_features=30, bias=True)\n",
       "  (lin2): Linear(in_features=30, out_features=40, bias=True)\n",
       "  (lin3): Linear(in_features=40, out_features=3, bias=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a00d670-0e3d-4db4-b675-3fdeb290054f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#파라메타 확인하기.\n",
    "Xtrain = torch.randn(60, 20)\n",
    "\n",
    "model = FCN()\n",
    "\n",
    "# model.lin1.weight.data = torch.ones(30,20, requires_grad = True)\n",
    "model.lin1.bias.data = torch.ones(60)\n",
    "model.lin1.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04b033-e7ee-4586-9a42-6bf6e4110384",
   "metadata": {},
   "source": [
    "#### My Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c55166a1-b9f3-4602-a42f-29b8d2cba1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Custom Model\n",
    "\n",
    "class MyNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # print(\"hello\")\n",
    "        \n",
    "        super().__init__() #super(MyNN, self).__init__() \n",
    "        #super 클래스의 __init__ 을 사용할 수 있게 초기화 시켜줌.\n",
    "        \n",
    "        self.lin1 = nn.Linear(20,30)\n",
    "        self.lin2 = nn.Linear(30,40)\n",
    "        self.lin3 = nn.Linear(40,3)\n",
    "        \n",
    "        self.relu = nn.ReLU(True)        \n",
    "\n",
    "    def forward(self, x): #forward pass를 할 때 사용하는 연산 정의\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        return x\n",
    "        #마지막 layer는 Not use Activation Function\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51184de9-5ed4-49b5-8828-bfc0220e6fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyNN(\n",
       "  (lin1): Linear(in_features=20, out_features=30, bias=True)\n",
       "  (lin2): Linear(in_features=30, out_features=40, bias=True)\n",
       "  (lin3): Linear(in_features=40, out_features=3, bias=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyNN()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "add793c2-acd8-400b-ab86-30b6ebe1348d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=20, out_features=30, bias=True)\n",
      "Linear(in_features=30, out_features=40, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.lin1)\n",
    "print(model.lin2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6555065-04ee-47ca-97c2-a0e70b7e78af",
   "metadata": {},
   "source": [
    "### 다시 수업으로 Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a24fa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Example of parameters() in models\n",
    "\n",
    "#상속 받을 때 super 초기화 후 forward에서 이미 계산이 다 됨.\n",
    "param_iterator = model.parameters()\n",
    "param_iterator\n",
    "\n",
    "for param in param_iterator:\n",
    "    # param.requires_grad = False\n",
    "    # print(param.requires_grad)\n",
    "    print(param.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e656c8-380c-43ac-b54b-f3e5bf3f1dfd",
   "metadata": {},
   "source": [
    "## nn.Sequential() 을 사용하여 Layer 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "929f4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential() example\n",
    "\n",
    "class FCN_seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                    nn.Linear(20, 30),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Linear(30, 40),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Linear(40, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "# self.lin1 = nn.Linear(20, 30)\n",
    "# self.lin2 = nn.Linear(30, 40)\n",
    "# self.lin3 = nn.Linear(40, 3)\n",
    "# self.relu = nn.ReLU(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "653f0bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN_seq(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=30, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=30, out_features=40, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=40, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seq = FCN_seq()\n",
    "Xtrain.shape\n",
    "model_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcaf7cf5-455c-4df9-a150-1a3a3f06d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_seq(Xtrain) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0db292b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=20, out_features=30, bias=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seq.fc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b067821-7aa3-48e3-8b6d-69cd0ba451a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLU(inplace=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seq.fc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ae10943-678c-4d0e-a80c-136f6038b333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=30, out_features=40, bias=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seq.fc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a16dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN_seq_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        \n",
    "        temp = self.fcn_block(20, 30)+self.fcn_block(30, 40)+[nn.Linear(40,1)] \n",
    "        # fan_block이라는 함수를 정의함. nn.Linear 와 nn.ReLU를 세트로 가짐. -> 간략화 가능.\n",
    "        \n",
    "        self.fc = nn.Sequential(*temp)\n",
    "        # *temp는 풀어서 나열.\n",
    "        \n",
    "        \n",
    "    def fcn_block(self, in_dim, out_dim):\n",
    "        return [nn.Linear(in_dim, out_dim), \n",
    "                             nn.ReLU(True)] #리스트로 만들어서 return\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "# self.lin1 = nn.Linear(20, 30)\n",
    "# self.lin2 = nn.Linear(30, 40)\n",
    "# self.lin3 = nn.Linear(40, 3)\n",
    "# self.relu = nn.ReLU(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ac30117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN_seq_v2(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=30, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=30, out_features=40, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=40, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seq_v2 = FCN_seq_v2()\n",
    "model_seq_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "514cccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN_final(nn.Module):\n",
    "    \n",
    "                      #  20     30,40      3 \n",
    "    def __init__(self, in_dim, hlayer, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        l_list = self.fcn_block(in_dim, hlayer[0]) #fan_block은 반환형이 list이므로 list를 안씌워줘도 됨.\n",
    "        \n",
    "        for l1, l2 in zip(hlayer[:-1], hlayer[1:]):\n",
    "            l_list = l_list + self.fcn_block(l1, l2)\n",
    "        \n",
    "        l_list = l_list + [nn.Linear(hlayer[-1], out_dim)] #Linear는 List가 아니라서 list를 씌워줘야 함.\n",
    "        \n",
    "        self.fc = nn.Sequential(*l_list)\n",
    "        \n",
    "    # fcn_block 이라는 사용자 정의 함수 만들기.     \n",
    "    def fcn_block(self, in_dim, out_dim):\n",
    "        return [nn.Linear(in_dim, out_dim),\n",
    "                             nn.ReLU(True)]\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6298ed12-9a63-4036-ba22-6e72468d420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZIP은 Iterator를 반환해줌.\n",
    "# 순서대로 리스트의 원소를 묶어서 반환 가능.\n",
    "\n",
    "# hlayer1 = [30, 40, 50, 60]\n",
    "# hlayer2 = [20, 80, 99, 100]\n",
    "\n",
    "# for i, j in zip(hlayer1, hlayer2):\n",
    "#   print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05a543cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlayer = [30, 40]\n",
    "in_dim = 20\n",
    "out_dim= 3\n",
    "\n",
    "myfcn_final = FCN_final(in_dim, hlayer, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35367025-26eb-48c2-a71c-5bc4682ed61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 40\n",
      "40 50\n",
      "50 60\n"
     ]
    }
   ],
   "source": [
    "hlayer = [30,40,50,60]\n",
    "# hlayer[:-1] = [30,40,50]\n",
    "# hlayer[1:]  = [40,50,60]\n",
    "\n",
    "for i,j in zip(hlayer[:-1], hlayer[1:]):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d545559c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN_final(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=30, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=30, out_features=40, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=40, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfcn_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d419c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordered dict example\n",
    "# nn.Sequential() example\n",
    "\n",
    "class FCN_seq_ordered_dic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(OrderedDict([('lin1', nn.Linear(20, 30)), # 이름 짓고 싶을 때 순서가 있는 OrderedDict 사용, 내용물은 list여야함.\n",
    "                      ('relu1', nn.ReLU(True)),\n",
    "                      ('lin2', nn.Linear(30, 40)),\n",
    "                      ('relu2',nn.ReLU(True)),\n",
    "                      ('lin3', nn.Linear(40, 3))\n",
    "                                            ])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "# self.lin1 = nn.Linear(20, 30)\n",
    "# self.lin2 = nn.Linear(30, 40)\n",
    "# self.lin3 = nn.Linear(40, 3)\n",
    "# self.relu = nn.ReLU(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a876e852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN_seq_ordered_dic(\n",
       "  (fc): Sequential(\n",
       "    (lin1): Linear(in_features=20, out_features=30, bias=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (lin2): Linear(in_features=30, out_features=40, bias=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (lin3): Linear(in_features=40, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이름만 불러도 되는 엄청난 친구... 대박.. 외워두기!!!\n",
    "# 남들에게 배포할 떄 이름 붙이기 때문에...! \n",
    "FCN_seq_ordered_dic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d185963c-89b7-49f8-bda4-b1f8ce890a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModuleList(), ModuleDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f025a1-5a48-44e6-82c3-0258183a04c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb4c2ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lin1.weight',\n",
       "              tensor([[ 0.1894, -0.1852, -0.1165,  0.0401, -0.2231,  0.0509, -0.0190, -0.2097,\n",
       "                       -0.0556, -0.1636,  0.0216,  0.1648,  0.2128, -0.2091,  0.0797, -0.1721,\n",
       "                        0.0795,  0.1231, -0.2130, -0.1614],\n",
       "                      [-0.2005, -0.0210,  0.0819, -0.1640, -0.0145, -0.0778,  0.2029, -0.2050,\n",
       "                       -0.1241, -0.0021, -0.1743,  0.1721, -0.2054, -0.0721, -0.0935, -0.0020,\n",
       "                       -0.1062,  0.1605, -0.1925, -0.1179],\n",
       "                      [-0.1023, -0.2137, -0.2218,  0.0789,  0.2106, -0.0342, -0.0868,  0.0978,\n",
       "                        0.1318, -0.2007, -0.1068,  0.1890,  0.0995, -0.2076,  0.0042, -0.1693,\n",
       "                       -0.1730, -0.1111,  0.2219, -0.1289],\n",
       "                      [ 0.1537, -0.2029, -0.0154,  0.2022, -0.0502, -0.0771, -0.0989, -0.1616,\n",
       "                        0.0717, -0.1630, -0.1422,  0.1942,  0.1664, -0.1169,  0.0047, -0.0601,\n",
       "                        0.1702, -0.2008, -0.2009,  0.1095],\n",
       "                      [ 0.0107, -0.0654,  0.1145,  0.0918, -0.1521,  0.1278,  0.1299, -0.0316,\n",
       "                       -0.1935,  0.0939,  0.1332, -0.2221, -0.2069, -0.0270, -0.1961, -0.1117,\n",
       "                        0.1745,  0.0409,  0.1336,  0.0527],\n",
       "                      [ 0.2186,  0.0925,  0.0658,  0.1693, -0.0888,  0.1916,  0.0213,  0.2047,\n",
       "                       -0.1333, -0.1519,  0.1627, -0.1288,  0.0432,  0.2118,  0.0418, -0.1588,\n",
       "                       -0.1361, -0.1234, -0.1517,  0.1199],\n",
       "                      [ 0.0324, -0.0201,  0.0623,  0.1307,  0.0197,  0.1452, -0.0300,  0.0864,\n",
       "                        0.0478,  0.0019,  0.1627,  0.2159,  0.1800, -0.0567, -0.0726,  0.1841,\n",
       "                       -0.1016,  0.0240, -0.1269, -0.0582],\n",
       "                      [-0.0835,  0.1396, -0.0978, -0.0971,  0.0948, -0.0124,  0.0216, -0.1672,\n",
       "                       -0.0984,  0.1124,  0.1027, -0.2050, -0.1553,  0.0650, -0.1756, -0.1183,\n",
       "                       -0.1265, -0.1707, -0.1092,  0.1569],\n",
       "                      [-0.2057,  0.1482, -0.1938, -0.0603, -0.0747,  0.1419, -0.1811, -0.1112,\n",
       "                       -0.2202, -0.0657,  0.0890,  0.2116,  0.1937, -0.0595, -0.1191,  0.0496,\n",
       "                        0.1483,  0.2003,  0.1530, -0.0040],\n",
       "                      [-0.1847, -0.1353,  0.1363, -0.1904, -0.1638, -0.1423, -0.0601,  0.1043,\n",
       "                        0.1550,  0.1723,  0.1038,  0.0508, -0.0588, -0.1370, -0.1139, -0.0067,\n",
       "                        0.1936, -0.0796, -0.0335, -0.0061],\n",
       "                      [-0.1006, -0.1063,  0.0635, -0.1596, -0.0135, -0.0540,  0.0496, -0.0286,\n",
       "                       -0.1172, -0.0675, -0.0554,  0.0952,  0.0510, -0.0694,  0.1753,  0.0199,\n",
       "                        0.1221, -0.0850, -0.1896, -0.2156],\n",
       "                      [-0.0745,  0.1378,  0.1406,  0.0681,  0.0632, -0.1502, -0.0660,  0.0271,\n",
       "                       -0.1951, -0.2034, -0.1696,  0.0693, -0.0284, -0.0395,  0.1391, -0.1496,\n",
       "                       -0.1383, -0.0511,  0.1378,  0.0359],\n",
       "                      [-0.1928,  0.1582, -0.1299,  0.1745,  0.0818,  0.0586,  0.1852,  0.0822,\n",
       "                       -0.2205,  0.0889,  0.2169, -0.0928,  0.1637,  0.1810, -0.0527,  0.0003,\n",
       "                       -0.0290,  0.1978,  0.0937,  0.1584],\n",
       "                      [-0.2001, -0.0907, -0.1689,  0.0489, -0.0873,  0.0648,  0.1164, -0.0591,\n",
       "                        0.0263, -0.1648, -0.0258,  0.0362,  0.1595,  0.0904,  0.0595,  0.1161,\n",
       "                        0.1749,  0.0210,  0.2154, -0.1363],\n",
       "                      [ 0.0757,  0.0007,  0.1655, -0.2134,  0.1085, -0.1923, -0.0669, -0.0721,\n",
       "                       -0.0748, -0.1086, -0.1823, -0.1957, -0.0456,  0.1826, -0.0600, -0.1511,\n",
       "                       -0.0638, -0.1844, -0.0121,  0.1837],\n",
       "                      [-0.2183,  0.1206,  0.1658,  0.1612, -0.1326,  0.0262,  0.1260, -0.1916,\n",
       "                        0.1718, -0.0164, -0.1968, -0.0574,  0.0064, -0.0911, -0.0908, -0.0230,\n",
       "                        0.0769,  0.1014, -0.2186, -0.0143],\n",
       "                      [ 0.0553,  0.1804,  0.1120,  0.1521,  0.0663, -0.0218, -0.1236, -0.0929,\n",
       "                        0.0723, -0.0468,  0.2059, -0.1402,  0.1520,  0.1432,  0.1477,  0.1753,\n",
       "                       -0.1293,  0.1292, -0.2214, -0.1200],\n",
       "                      [-0.0355,  0.1883, -0.0450, -0.1793, -0.1995,  0.0042,  0.0167,  0.0544,\n",
       "                       -0.0626,  0.0746,  0.0234, -0.0587,  0.1145, -0.1614, -0.1158,  0.0553,\n",
       "                       -0.0639, -0.0054, -0.0801,  0.1275],\n",
       "                      [ 0.2106,  0.1915,  0.0580, -0.1417, -0.0782, -0.1018, -0.0431, -0.1115,\n",
       "                        0.1333, -0.2121,  0.0098,  0.1616, -0.1774, -0.0392,  0.1226, -0.0781,\n",
       "                       -0.1059,  0.1191, -0.1548,  0.2231],\n",
       "                      [ 0.2006,  0.0100,  0.0361, -0.1139,  0.0521, -0.1689, -0.0583,  0.1788,\n",
       "                        0.0652,  0.1620,  0.1476,  0.2136,  0.0566, -0.1639, -0.2024,  0.1387,\n",
       "                        0.1816,  0.2202, -0.1519,  0.2043],\n",
       "                      [-0.0797,  0.0584,  0.1549, -0.1726, -0.2146,  0.1487, -0.2079,  0.1084,\n",
       "                        0.1294,  0.1636,  0.0054, -0.0571,  0.0292, -0.0310, -0.2107,  0.0528,\n",
       "                       -0.1312,  0.1655, -0.1050, -0.0436],\n",
       "                      [-0.1734,  0.2060,  0.1297,  0.1958,  0.1305, -0.1378,  0.2114, -0.0301,\n",
       "                        0.2057,  0.0313,  0.1416, -0.0888,  0.0878, -0.0013, -0.1785,  0.1732,\n",
       "                       -0.1217, -0.1482,  0.1794, -0.2051],\n",
       "                      [ 0.0772,  0.1818, -0.0356, -0.2115, -0.1812, -0.0111, -0.0355,  0.1553,\n",
       "                        0.1787, -0.1383, -0.0591, -0.1757,  0.0498,  0.1638, -0.0765,  0.0074,\n",
       "                       -0.0174, -0.1939,  0.2180,  0.0482],\n",
       "                      [ 0.2098, -0.1944, -0.0654, -0.0590,  0.0736, -0.0566, -0.1286,  0.1831,\n",
       "                        0.1755,  0.2224,  0.1344,  0.1471, -0.1477,  0.0167, -0.0701, -0.0200,\n",
       "                        0.0182,  0.0537, -0.1322, -0.0363],\n",
       "                      [-0.1032, -0.0575, -0.1440, -0.1168, -0.1562, -0.1570, -0.0147, -0.1901,\n",
       "                       -0.2132,  0.1569, -0.1610,  0.1565,  0.0155, -0.0327,  0.1730, -0.1746,\n",
       "                        0.1071,  0.1492,  0.1784, -0.2088],\n",
       "                      [ 0.0106, -0.0284, -0.1659,  0.1527, -0.0394,  0.1512, -0.1204, -0.0992,\n",
       "                        0.0841,  0.0286,  0.1282,  0.0954,  0.1790, -0.1648,  0.0505,  0.1661,\n",
       "                       -0.1013, -0.1397,  0.2200, -0.1873],\n",
       "                      [-0.1624, -0.1892,  0.0553,  0.0542, -0.1963,  0.1369,  0.0641,  0.0757,\n",
       "                       -0.2189, -0.2045, -0.1838, -0.0918, -0.1987,  0.1133, -0.0304, -0.0882,\n",
       "                        0.1943,  0.0295,  0.0289, -0.1256],\n",
       "                      [-0.0658, -0.1887, -0.1370,  0.2222, -0.1458,  0.1226,  0.1537,  0.1460,\n",
       "                        0.1831,  0.1919,  0.1200, -0.0120, -0.0573, -0.2135,  0.1177, -0.0507,\n",
       "                        0.0350, -0.0505,  0.1444,  0.0071],\n",
       "                      [ 0.1602,  0.1671,  0.0692, -0.0182,  0.1035, -0.1318,  0.1573,  0.1732,\n",
       "                        0.0291, -0.0225,  0.0553,  0.1968,  0.0528, -0.1132, -0.0479, -0.2198,\n",
       "                       -0.0060, -0.2039,  0.0504,  0.0286],\n",
       "                      [ 0.2082, -0.0668, -0.0117, -0.0008, -0.0821, -0.0794, -0.0924, -0.0944,\n",
       "                        0.0257,  0.0861,  0.0446,  0.1788, -0.0282,  0.0652, -0.1851,  0.1403,\n",
       "                        0.1852,  0.0856, -0.1439, -0.0083]])),\n",
       "             ('lin1.bias',\n",
       "              tensor([-0.1866, -0.2033,  0.1690,  0.1040,  0.1690,  0.1443, -0.0355,  0.1035,\n",
       "                      -0.1062, -0.1820, -0.0351, -0.1321,  0.2084, -0.1818,  0.2087, -0.0378,\n",
       "                       0.0405,  0.1947, -0.1035, -0.0829,  0.0925, -0.0567,  0.0283,  0.1409,\n",
       "                       0.0108,  0.1522, -0.0980,  0.0909,  0.0216, -0.1720])),\n",
       "             ('lin2.weight',\n",
       "              tensor([[-0.0204,  0.0713, -0.1124,  ..., -0.1678, -0.1528,  0.1277],\n",
       "                      [ 0.1142, -0.1618, -0.1572,  ...,  0.0087, -0.0518, -0.0072],\n",
       "                      [-0.0430, -0.1083,  0.0018,  ..., -0.0564,  0.0983,  0.1295],\n",
       "                      ...,\n",
       "                      [-0.1071,  0.0818, -0.1757,  ..., -0.0501,  0.1545, -0.1053],\n",
       "                      [-0.1422,  0.0405, -0.1250,  ...,  0.0839, -0.0084, -0.1397],\n",
       "                      [-0.1213,  0.0876, -0.1777,  ..., -0.1549,  0.0553,  0.1042]])),\n",
       "             ('lin2.bias',\n",
       "              tensor([-0.0850,  0.0878, -0.0199, -0.1279,  0.0822,  0.0922, -0.1679, -0.1583,\n",
       "                      -0.0211, -0.1429, -0.1388,  0.0285,  0.0138,  0.0660, -0.1086, -0.0835,\n",
       "                      -0.1767,  0.0992, -0.1586,  0.0341,  0.1364,  0.0717,  0.1059,  0.1536,\n",
       "                       0.0016, -0.0725,  0.0018, -0.1461, -0.1149,  0.0945,  0.1227,  0.0795,\n",
       "                      -0.1569, -0.1249,  0.1497,  0.0110, -0.1333,  0.0358,  0.1015,  0.1017])),\n",
       "             ('lin3.weight',\n",
       "              tensor([[ 0.1090, -0.0707,  0.0899, -0.1121,  0.0807, -0.1210,  0.0011,  0.1413,\n",
       "                        0.1150,  0.0719,  0.0107,  0.0873,  0.0017,  0.1293,  0.0882,  0.1560,\n",
       "                       -0.0515,  0.0830,  0.1168,  0.0769, -0.0892,  0.0582, -0.0781, -0.1386,\n",
       "                        0.1366, -0.1491,  0.1391, -0.0346, -0.0476, -0.1087, -0.0568,  0.0355,\n",
       "                       -0.0818,  0.1404,  0.0514,  0.1320, -0.1281, -0.0845, -0.0030, -0.0492],\n",
       "                      [-0.1094, -0.0670, -0.1398,  0.1408,  0.0888, -0.1326, -0.1059,  0.1400,\n",
       "                        0.1300,  0.0361, -0.0662,  0.0671,  0.0483, -0.0584,  0.0372,  0.1253,\n",
       "                        0.1277, -0.1323,  0.0581, -0.0096,  0.0693, -0.0763, -0.0601,  0.1211,\n",
       "                        0.1032,  0.0489,  0.1344, -0.1129,  0.1414, -0.1523,  0.0770,  0.0830,\n",
       "                        0.1146,  0.0942,  0.1084,  0.1267, -0.1443,  0.1398, -0.0587,  0.0482],\n",
       "                      [-0.1384, -0.1498,  0.1123, -0.1477,  0.0497,  0.1179,  0.1093, -0.0888,\n",
       "                        0.0131,  0.1109, -0.1508, -0.0199, -0.1325, -0.1435, -0.0763, -0.0671,\n",
       "                        0.1579,  0.0021, -0.0592,  0.0027,  0.0715,  0.1474,  0.0143,  0.1214,\n",
       "                        0.0130,  0.1016, -0.1414,  0.1241, -0.0546, -0.0667, -0.1062,  0.0655,\n",
       "                        0.1386,  0.0803,  0.0833, -0.0761, -0.1208,  0.0029, -0.0488,  0.0894]])),\n",
       "             ('lin3.bias', tensor([ 0.0440, -0.0138, -0.1153]))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict() example\n",
    "\n",
    "# 내 모델, 모델에 대한 이름, 모델에 해당하는 파라메타, bias , Layer 별 Weight and bias\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c586f6",
   "metadata": {},
   "source": [
    "# Problem Setup\n",
    "\n",
    "<div style=\"text-align:center\"> <img src='https://drive.google.com/uc?export=download&id=1FRhniwGeeutBSJQRdW6GzshMfDrPz7oJ' width=\"250\" height=\"200\"> </div>\n",
    "    \n",
    "Build a Fully connected neural network with\n",
    "\n",
    "- 3 layers\n",
    "- 마지막 layer의 unit 수는 `1` \n",
    "  - 마지막 layer의 activation은 없음 (linear layer)\n",
    "- Data feature 수는 `100`\n",
    "\n",
    "- input unit 수는 data 크기를 보고 맞추세요\n",
    "- hidden layer의 unit 수는 `[80, 50]`\n",
    "  - hidden layer의 activation 함수는 ReLU\n",
    "\n",
    "- model class 명 `myFCN`\n",
    "  - instance 명 `my_model` 생성\n",
    "  - `my_model` 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d8150a-5d25-40fe-951a-416a5f022112",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "problem setup에서 구성한 neural network을 `nn.Sequential`을 활용하여 생성하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3dabeb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 사용할 data \n",
    "batch_size = 30\n",
    "num_feature = 100\n",
    "\n",
    "X_train = torch.randn(batch_size, num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc5c673c-068b-4201-8900-3669a84d420a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 100])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82fc8c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1 코딩 (매 줄마다 주석 필수 )\n",
    "\n",
    "class myFCN(nn.Module):                     # n .Module을 상속받은 myFCN 이라는 이름의 class를 정의합니다. super class : nn.Module, sub class : myFCN\n",
    "    def __init__(self):                     # __init__(self) init는 초기화 함수입니다.\n",
    "        super().__init__()                  # super().__init__() 를 통해 super class인 nn.Module을 초기화 하여 사용 가능하게 만듭니다.\n",
    "\n",
    "        self.fc = nn.Sequential(           # self.fc는 nn.Sequential로 만sd a들어진 Neural Network 입니다. fc는 fully connnected Layer\n",
    "                   nn.Linear(100, 80),     # Hidden Layer의 unit수가 [80, 50] 이고 Input data는 100 이므로 첫번째 Layer는 nn.Linear(100,80)\n",
    "                   nn.ReLU(True),          # 마지막 Output Layer를 제외하고는 Activation Function으로 ReLU를 넣는다. \n",
    "                                    # True를 쓰게 되면 메모리 위치를 두고 그 위치에서 연산 수행, inplace 연산. (default = False), 메모리 절약\n",
    "                   nn.Linear(80,50),       # 이 전 Layer의 shape는 (100,80)이므로 이번 Layer의 Input은 80이여야 한다. 또한 2번째 Hidden Layer의 Unit이 50이므로 (80,50)\n",
    "                   nn.ReLU(True),          # 마지막 Output Layer가 아니므로 Activation Function으로 ReLU 사용.\n",
    "                   nn.Linear(50,1)         # 이 전 Hidden Layer의 unit이 50, 마지막 Layer의 unit수는 1이므로 마지막 Layer의 shape는 (50,1)\n",
    "        )\n",
    "          \n",
    "    def forward(self, x):                  # forward pass 진행.\n",
    "        return self.fc(x)                  # fully connected layer에 입력 데이터인 X가 들어감. x의 shape는 (n, 100)이 되야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "846477b3-37df-43a8-874c-f6c903e797c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myFCN(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=80, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=80, out_features=50, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = myFCN() # myFCN 클래스로 만들어진 fully connected Neural Network를 를 my_model 이라는 변수에 집어 넣습니다.\n",
    "my_model           # my_model을 출력합니다. 출력 결과는 myFCN 클래스에서 직접 만든 Neural Network 형태가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b56aa9c-8430-44fa-a986-6e3218c2752c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1542, -0.0652, -0.0659, -0.1898, -0.1814, -0.1381],\n",
       "        [-0.1315, -0.1371, -0.0740, -0.0801, -0.0218, -0.1093],\n",
       "        [-0.0469, -0.2036, -0.0678, -0.1444, -0.0597, -0.1447],\n",
       "        [-0.1343, -0.0840, -0.1385, -0.1610, -0.0923, -0.1432],\n",
       "        [-0.0286, -0.0775, -0.1954, -0.1301, -0.1171, -0.0879]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "676dc450-c946-4fa5-a3e8-f1fb45455b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1315, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm = my_model(X_train).reshape(5,6)\n",
    "mm[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a46397a2-bf43-43cd-9c2c-fb1143009f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1542],\n",
       "        [-0.0652],\n",
       "        [-0.0659],\n",
       "        [-0.1898],\n",
       "        [-0.1814],\n",
       "        [-0.1381],\n",
       "        [-0.1315],\n",
       "        [-0.1371],\n",
       "        [-0.0740],\n",
       "        [-0.0801],\n",
       "        [-0.0218],\n",
       "        [-0.1093],\n",
       "        [-0.0469],\n",
       "        [-0.2036],\n",
       "        [-0.0678],\n",
       "        [-0.1444],\n",
       "        [-0.0597],\n",
       "        [-0.1447],\n",
       "        [-0.1343],\n",
       "        [-0.0840],\n",
       "        [-0.1385],\n",
       "        [-0.1610],\n",
       "        [-0.0923],\n",
       "        [-0.1432],\n",
       "        [-0.0286],\n",
       "        [-0.0775],\n",
       "        [-0.1954],\n",
       "        [-0.1301],\n",
       "        [-0.1171],\n",
       "        [-0.0879]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fc48cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Problem 2\n",
    "\n",
    "problem setup에서 구성한 neural network을 `OrderedDict`을 활용하여 생성하세요\n",
    "- 각 layer의 이름을 주고 생성하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "50011c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "0b7a32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myFCN(nn.Module): # myFCN2라는 이름의 클래스를 만듭니다. nn.Module을 상속받아 nn.Module의 여러 속성들을 사용\n",
    "    def __init__(self): # __init__ 을 통해 초기화 진행.\n",
    "        super().__init__() # super class 즉 nn.Module을 사용하기 위해 초기화를 진행합니다. __init__()를 통해 초기화 진행.\n",
    "    \n",
    "        self.fc = nn.Sequential(OrderedDict([ #OrderDict를 사용하여 모델의 Layer 쌓기, OrderDict를 사용하면 각 Layer에 이름을 줄 수 있음.\n",
    "                        ('lin1', nn.Linear(100,80)), # lin1 이라는 이름의 Layer는 입력 Unit이 100이 되며 다음 Hidden Layer의 Unit이 80이므로 (100,80)이 됩니다.\n",
    "                        ('ReLU1', nn.ReLU(True)),    # ReLU1 이라는 이름의 Layer는 lin1 layer에 activation 함수로 ReLU 를 적용합니다. (마지막 레이어가 아니므로 Activation Function 사용)\n",
    "                        ('lin2', nn.Linear(80,50)),  # lin2 이라는 이름의 layer는 입력 Unit이 80, 다음층의 Unit이 50이므로 (80,50) 이 됨.\n",
    "                        ('ReLU2', nn.ReLU(True)),    # ReLU2 이라는 이름의 Layer는 lin2 layer에 activation 함수로 ReLU 를 적용.\n",
    "                        ('lin3', nn.Linear(50,1))    # lin3이라는 이름의 Layer는 마지막 Layer, 입력 Unit의 수가 50, 출력 Unit 수가 1이므로 (50,1) 이 됨. \n",
    "                                                     #  마지막 Layer는 Activation Function 사용 안하기 때문에 이 상태로 종료.\n",
    "                        ]))\n",
    "                               \n",
    "    \n",
    "    def forward(self, x):  # forward pass 진행.\n",
    "        return self.fc(x)  # fully connected layer에 입력 데이터인 X가 들어감. x의 shape는 (n, 100)이 되야함.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "e3659cdb-bbb3-4857-89dc-d8c571140a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myFCN(\n",
       "  (fc): Sequential(\n",
       "    (lin1): Linear(in_features=100, out_features=80, bias=True)\n",
       "    (ReLU1): ReLU(inplace=True)\n",
       "    (lin2): Linear(in_features=80, out_features=50, bias=True)\n",
       "    (ReLU2): ReLU(inplace=True)\n",
       "    (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my_model2 = myFCN2()\n",
    "# my_model2\n",
    "\n",
    "myFCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "cd6de676-26c3-4721-962b-d64e5fb75afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1076e-02],\n",
       "        [ 1.2121e-01],\n",
       "        [ 1.7963e-01],\n",
       "        [ 3.0297e-02],\n",
       "        [-1.2761e-02],\n",
       "        [-1.2974e-01],\n",
       "        [ 8.2657e-02],\n",
       "        [ 6.4025e-03],\n",
       "        [ 5.2668e-02],\n",
       "        [ 5.0146e-02],\n",
       "        [ 9.9481e-02],\n",
       "        [ 3.7431e-03],\n",
       "        [ 7.5810e-02],\n",
       "        [-7.7921e-02],\n",
       "        [ 7.1664e-03],\n",
       "        [ 1.8505e-02],\n",
       "        [ 1.2403e-01],\n",
       "        [ 4.1419e-02],\n",
       "        [-4.9096e-02],\n",
       "        [ 2.3271e-02],\n",
       "        [ 2.7100e-02],\n",
       "        [-2.9133e-02],\n",
       "        [-6.8623e-02],\n",
       "        [ 2.0562e-02],\n",
       "        [ 3.1696e-02],\n",
       "        [-1.0196e-02],\n",
       "        [ 2.0711e-02],\n",
       "        [-1.4658e-04],\n",
       "        [ 4.7573e-02],\n",
       "        [-4.9960e-02]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model2(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a6dc9-cf77-4d63-aa50-8908cffbda69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
