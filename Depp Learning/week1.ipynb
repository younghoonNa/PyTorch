{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3cac1c9-0817-49d1-b45f-df2305485f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "#linspace  -> Lineary spaced 의 줄임말로 1차원 배열을 만들 떄 주로 쓰임\n",
    "#np.linspace(start, stop, num) \n",
    "#math.pi = 3.141592 ....\n",
    "\n",
    "y = np.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "309d919c-4b48-4231-aa9e-ac5145d039c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba0812a-9858-4adf-a4d8-5296a37ac2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "730547ae-7c6b-48fa-b1b5-892f2a8754ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2277.345033524459\n",
      "199 1521.735258641615\n",
      "299 1018.331067534099\n",
      "399 682.7956023190541\n",
      "499 459.0408193333876\n",
      "599 309.7512011203054\n",
      "699 210.0910141556679\n",
      "799 143.52380594503117\n",
      "899 99.03429996884063\n",
      "999 69.28164532409662\n",
      "1099 49.37136877405993\n",
      "1199 36.0384563943432\n",
      "1299 27.103718896310724\n",
      "1399 21.111873636625027\n",
      "1499 17.090499992076637\n",
      "1599 14.38942694693878\n",
      "1699 12.573663631190193\n",
      "1799 11.351987872257673\n",
      "1899 10.529292457684758\n",
      "1999 9.974767747022145\n"
     ]
    }
   ],
   "source": [
    "for t in range(2000):\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    # y = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    #순전파 단계 : 예측값 y를 계산하고 출력\n",
    "\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    # 손실 loss 를 계산하고 출력.\n",
    "    \n",
    "    if t%100 == 99:\n",
    "        print(t, loss)\n",
    "        \n",
    "    #손실에 따른 a,b,c,d 의 변화도 (gradient)를 계산하고 역전파 합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "    \n",
    "    #가중치 갱신\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6b65663-63f4-4c21-be09-e7c5f59ca51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: y = 0.020855726068319702 + 0.8297773662880767 x + -0.0035979597875649004 x^2 + -0.08949506926785358 x^3\n"
     ]
    }
   ],
   "source": [
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08babc2d-4da3-430c-82ba-90f6461d182c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020855726068319702\n",
      "0.8297773662880767\n",
      "-0.0035979597875649004\n",
      "-0.08949506926785358\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3076927-f7c6-42d4-92c9-632457dd1f7c",
   "metadata": {},
   "source": [
    "## pyTorch에서 신경망 구성하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be8a2a20-3a2f-45fa-b8c7-e928d5517c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7405e34-48a8-4092-b7c3-b897efc54f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "#device = torch.device(\"cuda:0\") #GPU에서 실행하려면 이 주석 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "486ae0c7-93e6-489d-9c27-95bf3b95cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype = dtype)\n",
    "y = torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59e82c5e-4872-4650-b818-25c27dc449d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#무작위로 가중치 초기화\n",
    "a = torch.randn((), device = device, dtype = dtype)\n",
    "b = torch.randn((), device = device, dtype = dtype)\n",
    "c = torch.randn((), device = device, dtype = dtype)\n",
    "d = torch.randn((), device = device, dtype = dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f4214aa-da9c-440d-958d-51ea7fa5298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6f4a857-8b35-491a-a838-13507aa0980d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 775.0797119140625\n",
      "199 531.5973510742188\n",
      "299 365.8927001953125\n",
      "399 252.99142456054688\n",
      "499 175.97952270507812\n",
      "599 123.38863372802734\n",
      "699 87.43299102783203\n",
      "799 62.8226203918457\n",
      "899 45.95827102661133\n",
      "999 34.388675689697266\n",
      "1099 26.44245147705078\n",
      "1199 20.978652954101562\n",
      "1299 17.21755027770996\n",
      "1399 14.625694274902344\n",
      "1499 12.837644577026367\n",
      "1599 11.60279655456543\n",
      "1699 10.749092102050781\n",
      "1799 10.158291816711426\n",
      "1899 9.749024391174316\n",
      "1999 9.465215682983398\n"
     ]
    }
   ],
   "source": [
    "for t in range(2000):\n",
    "    #순전파 단계의 예측값 y를 계산\n",
    "    y_pred = a + b * x + c  * x ** 2 + d * x ** 3\n",
    "    \n",
    "    #손실(loss)를 게산하고 출력합니다.\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    \n",
    "    if t%100 == 99:\n",
    "        print(t, loss)\n",
    "        \n",
    "        \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    \n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "    \n",
    "    #이제 가중치를 갱신\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6915cf21-664b-4c76-924c-533c9bd758a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: y = -0.023334583267569542 + 0.8691157102584839 x + 0.004025603178888559 x^2 + -0.09509061276912689 x^3\n"
     ]
    }
   ],
   "source": [
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a2a8c59-105e-48c4-81ef-d5c870f69824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a :  tensor(-0.0233) \n",
      "\n",
      "b :  tensor(0.8691) \n",
      "\n",
      "c :  tensor(0.0040) \n",
      "\n",
      "d :  tensor(-0.0951) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"a : \" , a , \"\\n\")\n",
    "print(\"b : \" , b , \"\\n\")\n",
    "print(\"c : \" , c , \"\\n\")\n",
    "print(\"d : \" , d , \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80aa365-d0ae-4124-b298-b4e503ea2ac1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pytorch의 기본 문법 따라하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e9f396a-34e4-403f-96a6-e01a90167adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1286.8916015625\n",
      "199 856.3814086914062\n",
      "299 570.9948120117188\n",
      "399 381.77783203125\n",
      "499 256.3014221191406\n",
      "599 173.0777587890625\n",
      "699 117.86752319335938\n",
      "799 81.23263549804688\n",
      "899 56.91870880126953\n",
      "999 40.77760314941406\n",
      "1099 30.059486389160156\n",
      "1199 22.940431594848633\n",
      "1299 18.210529327392578\n",
      "1399 15.06694221496582\n",
      "1499 12.977056503295898\n",
      "1599 11.587143898010254\n",
      "1699 10.66242790222168\n",
      "1799 10.046998023986816\n",
      "1899 9.637224197387695\n",
      "1999 9.36426830291748\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device = device, dtype = dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "a = torch.randn((), device = device, dtype = dtype, requires_grad = True)\n",
    "b = torch.randn((), device = device, dtype = dtype, requires_grad = True)\n",
    "c = torch.randn((), device = device, dtype = dtype, requires_grad = True)\n",
    "d = torch.randn((), device = device, dtype = dtype, requires_grad = True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    loss.backward()\n",
    "    #requires_grad = True를 가지는 모든 텐서에 대해서 손실의 변화도를 게산합니다.\n",
    "\n",
    "    \n",
    "    # 경사하강법(gradient descent)를 사용하여 가중치를 직접 갱신.\n",
    "    # torch.no_grad()로 감싸는 이유는, 가중치들이 requires_grad=True 지만\n",
    "    # autograd에서는 이를 추적하지 않을 것이기 때문입니다.\n",
    "    # -> autograd 즉 자동으로 오차를 개선해주는 과정에서는 grad를 게산할 필요가 없음.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "        \n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00881cf4-e1d5-4a11-97a5-20c8470c8d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: y = -0.009082827717065811 + 0.8778927326202393 x + 0.0015669402200728655 x^2 + -0.09633906185626984 x^3\n"
     ]
    }
   ],
   "source": [
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17061f87-c63b-43ad-b3d7-aa3b768172fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a :  tensor(-0.0091, requires_grad=True) \n",
      "\n",
      "b :  tensor(0.8779, requires_grad=True) \n",
      "\n",
      "c :  tensor(0.0016, requires_grad=True) \n",
      "\n",
      "d :  tensor(-0.0963, requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"a : \" , a , \"\\n\")\n",
    "print(\"b : \" , b , \"\\n\")\n",
    "print(\"c : \" , c , \"\\n\")\n",
    "print(\"d : \" , d , \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18983055-8f50-4f65-b035-70d6fd3378bb",
   "metadata": {},
   "source": [
    "# Pytorch에서 새 autograd Function 정의하기.\n",
    "\n",
    "- forward() : 입력 텐서로 부터 출력 텐서를 계산\n",
    "- backward() : 어떤 스칼라 값에 대한 출력 텐서의 변화도를 전달받고 동일한 스칼라 값에 대한 입력 텐서의 변화도를 계산.\n",
    "\n",
    "> How to use\n",
    ">> `torch.autograd.Function` 의 하위 클래스를 정의하고 `forward` 와 `backward` 함수를 구현 하여 사용자 정의 autograd Function 사용가능\n",
    "\n",
    "- 이번 에제에서는 위에서 다룬 다항식과 다른 0.5 x 5x^3  - 3x 를 이용한 3차 르장드르 다항식을 이용하여 순전파와 역전파 사용법을 알아보자.\n",
    "\n",
    "\\begin{align*}\n",
    "y = a + b𝑥 + {c𝑥}^2 + {d𝑥}^3  \\rightarrow P(𝑥) =  0.5 * ( {5𝑥}^3  - 3𝑥 ) \n",
    "\\end{align*}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5814835c-6756-4520-a4a9-d049b71c7a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    torch.autograd.Function을 상속받아 사용자 정의 autograd Function을 구현하고,\n",
    "    텐서 연산을 하는 순전파 단계와 역전파 단계를 구현\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        순전파 단계(forward)에서는 입력을 갖는 텐서를 받아 출력 텐서를 반환.\n",
    "        ctx는 컨텍스트 객체(context object)로 역전파 연산을 위한 정보 저장에 사용\n",
    "        \n",
    "        ctx.save_for_backward 메소드를 사용하여 역전파 단계에서 사용할 어떤 객체도 저장(cache) 해 둘 수 있다.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3*input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\" \n",
    "        역전파 단계에서는 출력에 대한 손실(loss)의 변화도(gradient)를 갖는 텐서를 받고, \n",
    "        입력에 대한 손실의 변화도를 계산해야 합니다.\n",
    "        \n",
    "        -> 지금까지 위에서 정의하고 loss 개선 했던 부분을 backward로 바꿈.\n",
    "        \"\"\"\n",
    "        \n",
    "        input , = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8c1efc3-5bb4-4062-99d8-21bbc3fd02b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 209.95834350585938\n",
      "199 144.66018676757812\n",
      "299 100.70249938964844\n",
      "399 71.03519439697266\n",
      "499 50.97850799560547\n",
      "599 37.403133392333984\n",
      "699 28.206867218017578\n",
      "799 21.97318458557129\n",
      "899 17.7457275390625\n",
      "999 14.877889633178711\n",
      "1099 12.93176555633545\n",
      "1199 11.610918045043945\n",
      "1299 10.714258193969727\n",
      "1399 10.10548210144043\n",
      "1499 9.692106246948242\n",
      "1599 9.411375999450684\n",
      "1699 9.220744132995605\n",
      "1799 9.091285705566406\n",
      "1899 9.003361701965332\n",
      "1999 8.943639755249023\n",
      "Result: y = -7.290119619085544e-09 + -2.208526849746704 * P3(1.3728043146699065e-09 + 0.2554861009120941 x)\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device = device, dtype = dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "\n",
    "#가중치를 가지는 임이의 텐서 만들기\n",
    "# y = a + b + P3(c+d*x)\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "    #사용자 정의 Function을 사용하기 위해 Function.apply() 함수를 사용\n",
    "    \n",
    "    #순전파 단계 \n",
    "    #사용자 정의 autograd 연산을 사용하여 p3을 게산\n",
    "    y_pred = a + b * P3(c+d*x)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t%100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    #autograd를 사용하여 역전파 단계를 게산.\n",
    "    loss.backward()\n",
    "    \n",
    "    #경사하강법 (Gradient Descient)를 사용하여 가중치를 갱신\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # 가중치 갱신 후에는 변화도를 직접 0으로 만듭니다.\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a890535b-fe9d-4dc0-95f9-0640ceef8191",
   "metadata": {},
   "source": [
    "# nn Module 을 사용하여 모델 생성하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67da55db-ce34-42f4-8181-c14ba24a356b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 665.7625732421875\n",
      "199 450.2120666503906\n",
      "299 305.6358947753906\n",
      "399 208.58900451660156\n",
      "499 143.3936004638672\n",
      "599 99.55874633789062\n",
      "699 70.06051635742188\n",
      "799 50.192100524902344\n",
      "899 36.79756164550781\n",
      "999 27.758813858032227\n",
      "1099 21.65342140197754\n",
      "1199 17.525297164916992\n",
      "1299 14.731201171875\n",
      "1399 12.83807373046875\n",
      "1499 11.553980827331543\n",
      "1599 10.682077407836914\n",
      "1699 10.089404106140137\n",
      "1799 9.686071395874023\n",
      "1899 9.411291122436523\n",
      "1999 9.223886489868164\n",
      "Result: y = -0.015379405580461025 +      0.8431451916694641 x +      0.0026532020419836044 x^2 +      -0.0913965255022049 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "p = torch.tensor([1,2,3])\n",
    "xx= x.unsqueeze(-1).pow(p)\n",
    "# 위 코드에서, x.unsqueeze(-1)은 (2000, 1)의 shape을, p는 (3,)의 shape을 가지므로,\n",
    "# 이 경우 브로드캐스트(broadcast)가 적용되어 (2000, 3)의 shape을 갖는 텐서를 얻습니다.\n",
    "# 2000, * 3 -> 2000, 3\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# loss function으로는 mse 사용\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "\n",
    "    y_pred = model(xx)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "        \n",
    "linear_layer = model[0]\n",
    "\n",
    "# 선형 계층에서, 매개변수는 `weights` 와 `bias` 로 저장됩니다.\n",
    "print(f'Result: y = {linear_layer.bias.item()} +\\\n",
    "      {linear_layer.weight[:, 0].item()} x +\\\n",
    "      {linear_layer.weight[:, 1].item()} x^2 +\\\n",
    "      {linear_layer.weight[:, 2].item()} x^3')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b3153-5a29-4ae4-8eae-780abb75d78c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
